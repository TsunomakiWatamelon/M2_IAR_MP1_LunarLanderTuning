{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "from torch.optim import Adam\n",
    "import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting easypip\n",
      "  Downloading easypip-1.3.5-py3-none-any.whl.metadata (475 bytes)\n",
      "Requirement already satisfied: packaging in /home/hocine/ENTER/envs/deepdac/lib/python3.9/site-packages (from easypip) (24.1)\n",
      "Downloading easypip-1.3.5-py3-none-any.whl (3.8 kB)\n",
      "Installing collected packages: easypip\n",
      "Successfully installed easypip-1.3.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[easypip] Installing swig\n",
      "[easypip] Installing bbrl_utils\n",
      "/home/hocine/ENTER/envs/deepdac/lib/python3.9/site-packages/bbrl_utils/notebook.py:46: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm  # noqa: F401\n"
     ]
    }
   ],
   "source": [
    "# Prepare the environment\n",
    "try:\n",
    "    from easypip import easyimport\n",
    "except ModuleNotFoundError:\n",
    "    from subprocess import run\n",
    "\n",
    "    assert (\n",
    "        run([\"pip\", \"install\", \"easypip\"]).returncode == 0\n",
    "    ), \"Could not install easypip\"\n",
    "    from easypip import easyimport\n",
    "\n",
    "easyimport(\"swig\")\n",
    "easyimport(\"bbrl_utils\").setup(maze_mdp=True)\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "import bbrl_gymnasium  # noqa: F401\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from bbrl.agents import Agent, Agents, TemporalAgent\n",
    "from bbrl_utils.nn import build_mlp, setup_optimizer, soft_update_params\n",
    "from bbrl_utils.notebook import setup_tensorboard\n",
    "from omegaconf import OmegaConf\n",
    "from bbrl_utils.algorithms import EpochBasedAlgo\n",
    "from bbrl.visu.plot_policies import plot_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD3 implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3(EpochBasedAlgo):\n",
    "    def __init__(self, cfg):\n",
    "        # Appel du constructeur de la classe parente EpochBasedAlgo\n",
    "        super().__init__(cfg)\n",
    "\n",
    "        # Récupère la taille des observations et des actions depuis l'environnement d'entraînement\n",
    "        obs_size, act_size = self.train_env.get_obs_and_actions_sizes()\n",
    "\n",
    "        # Initialisation du premier critique (critic_1) : réseau Q qui prend en entrée l'état et l'action\n",
    "        # Il est utilisé pour estimer la valeur Q d'une action donnée dans un état donné.\n",
    "        self.critic_1 = tools.ContinuousQAgent(\n",
    "            obs_size, cfg.algorithm.architecture.critic_hidden_size, act_size\n",
    "        ).with_prefix(\"critic-1/\")  # On donne un préfixe pour identifier ce réseau dans les logs\n",
    "        \n",
    "        # Initialisation du second critique (critic_2) : deuxième réseau Q\n",
    "        # Le second critique est une copie du premier mais est utilisé pour atténuer les biais d'estimation.\n",
    "        self.critic_2 = tools.ContinuousQAgent(\n",
    "            obs_size, cfg.algorithm.architecture.critic_hidden_size, act_size\n",
    "        ).with_prefix(\"critic-2/\")\n",
    "\n",
    "        # Création des critiques cibles (target_critic_1 et target_critic_2) par copie des critiques originaux.\n",
    "        # Les critiques cibles sont utilisés pour calculer les valeurs cibles de Q lors de l'apprentissage.\n",
    "        self.target_critic_1 = copy.deepcopy(self.critic_1).with_prefix(\"target-critic-1/\")\n",
    "        self.target_critic_2 = copy.deepcopy(self.critic_2).with_prefix(\"target-critic-2/\")\n",
    "\n",
    "        # Initialisation de l'acteur : réseau déterministe qui prend en entrée l'état et produit l'action correspondante.\n",
    "        self.actor = tools.ContinuousDeterministicActor(\n",
    "            obs_size, cfg.algorithm.architecture.actor_hidden_size, act_size\n",
    "        )\n",
    "\n",
    "        # Ajout de bruit gaussien à l'action pour favoriser l'exploration durant l'entraînement\n",
    "        noise_agent = tools.AddGaussianNoise(cfg.algorithm.action_noise)\n",
    "\n",
    "        # Définition de la politique d'entraînement : l'acteur avec du bruit pour l'exploration\n",
    "        self.train_policy = Agents(self.actor, noise_agent)\n",
    "        # Définition de la politique d'évaluation : acteur sans bruit (politique déterministe)\n",
    "        self.eval_policy = self.actor\n",
    "\n",
    "        # Création d'agents temporels pour traiter les informations au fil du temps.\n",
    "        # Ils permettent d'appliquer les réseaux sur des séquences de données.\n",
    "        self.t_actor = TemporalAgent(self.actor)\n",
    "        self.t_critic_1 = TemporalAgent(self.critic_1)\n",
    "        self.t_critic_2 = TemporalAgent(self.critic_2)\n",
    "        self.t_target_critic_1 = TemporalAgent(self.target_critic_1)\n",
    "        self.t_target_critic_2 = TemporalAgent(self.target_critic_2)\n",
    "\n",
    "        # Configuration des optimizers pour l'acteur et les critiques\n",
    "        self.actor_optimizer = setup_optimizer(cfg.actor_optimizer, self.actor)\n",
    "        self.critic_optimizer = setup_optimizer(cfg.critic_optimizer, self.critic_1, self.critic_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_td3(td3: TD3):\n",
    "    # Boucle principale d'apprentissage TD3, qui itère sur le replay buffer (mémoire d'expériences)\n",
    "    for rb in td3.iter_replay_buffers():\n",
    "        # Extraction d'un batch d'expériences aléatoires depuis le replay buffer\n",
    "        rb_workspace = rb.get_shuffled(td3.cfg.algorithm.batch_size)\n",
    "\n",
    "        # Mise à jour des réseaux de critiques à t=0\n",
    "        td3.t_critic_1(rb_workspace, t=0, n_steps=1)  # Calcul des valeurs Q pour critic_1\n",
    "        td3.t_critic_2(rb_workspace, t=0, n_steps=1)  # Calcul des valeurs Q pour critic_2\n",
    "        \n",
    "        # Avec torch.no_grad() pour désactiver la backpropagation (pas de mise à jour ici)\n",
    "        with torch.no_grad():\n",
    "            # Application de l'acteur pour générer les actions futures à t=1\n",
    "            td3.t_actor(rb_workspace, t=1, n_steps=1)\n",
    "            # Calcul des valeurs Q cibles à t=1\n",
    "            td3.t_target_critic_1(rb_workspace, t=1, n_steps=1)\n",
    "            td3.t_target_critic_2(rb_workspace, t=1, n_steps=1)\n",
    "\n",
    "        # Récupération des informations nécessaires du workspace pour les mises à jour\n",
    "        q_values_1, q_values_2, terminated, reward, target_q_values_1, target_q_values_2 = rb_workspace[\n",
    "            \"critic-1/q_value\", \"critic-2/q_value\", \"env/terminated\", \"env/reward\", \"target-critic-1/q_value\", \"target-critic-2/q_value\"\n",
    "        ]\n",
    "        # Calcul du masque pour identifier les transitions à bootstrapper (non terminées)\n",
    "        must_bootstrap = ~terminated\n",
    "\n",
    "        # Calcul des valeurs Q cibles : on prend le minimum des valeurs Q des deux critiques cibles (principe clé de TD3 pour éviter la surestimation des valeurs Q)\n",
    "        target_q_values = torch.min(target_q_values_1, target_q_values_2)\n",
    "\n",
    "        # --- Mise à jour des critiques ---\n",
    "        # Calcul de la perte pour critic_1 en utilisant la fonction compute_critic_loss\n",
    "        critic_loss_1 = tools.compute_critic_loss(\n",
    "            td3.cfg, reward, must_bootstrap, q_values_1, target_q_values\n",
    "        )\n",
    "        # Calcul de la perte pour critic_2\n",
    "        critic_loss_2 = tools.compute_critic_loss(\n",
    "            td3.cfg, reward, must_bootstrap, q_values_2, target_q_values\n",
    "        )\n",
    "        # La perte totale des critiques est la somme des deux\n",
    "        critic_loss = critic_loss_1 + critic_loss_2\n",
    "\n",
    "        # Enregistrement de la perte des critiques dans les logs\n",
    "        td3.logger.add_log(\"critic_loss\", critic_loss, td3.nb_steps)\n",
    "\n",
    "        # Gradient step (mise à jour) pour les critiques\n",
    "        td3.critic_optimizer.zero_grad()  # Réinitialisation des gradients\n",
    "        critic_loss.backward()  # Calcul des gradients via backpropagation\n",
    "        # Application du clipping de gradient pour éviter des mises à jour trop grandes (stabilité)\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            td3.critic_1.parameters(), td3.cfg.algorithm.max_grad_norm\n",
    "        )\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            td3.critic_2.parameters(), td3.cfg.algorithm.max_grad_norm\n",
    "        )\n",
    "        # Application de la mise à jour des critiques\n",
    "        td3.critic_optimizer.step()\n",
    "\n",
    "        # --- Mise à jour de l'acteur ---\n",
    "        # Calcul des actions en utilisant l'acteur à t=0\n",
    "        td3.t_actor(rb_workspace, t=0, n_steps=1)\n",
    "        # Mise à jour du critic_1 avec ces actions\n",
    "        td3.t_critic_1(rb_workspace, t=0, n_steps=1)\n",
    "\n",
    "        # Récupération des valeurs Q issues de critic_1\n",
    "        q_values = rb_workspace[\"critic-1/q_value\"]\n",
    "        # Calcul de la perte de l'acteur : l'objectif est de maximiser les valeurs Q sous la politique actuelle\n",
    "        actor_loss = tools.compute_actor_loss(q_values)\n",
    "\n",
    "        # Gradient step pour l'acteur\n",
    "        td3.actor_optimizer.zero_grad()  # Réinitialisation des gradients\n",
    "        actor_loss.backward()  # Calcul des gradients via backpropagation\n",
    "        # Clipping de gradient pour l'acteur\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            td3.actor.parameters(), td3.cfg.algorithm.max_grad_norm\n",
    "        )\n",
    "        # Application de la mise à jour de l'acteur\n",
    "        td3.actor_optimizer.step()\n",
    "\n",
    "        # --- Mise à jour des cibles critiques avec soft update ---\n",
    "        # Application de la mise à jour douce (soft update) des paramètres de critic_1 et target_critic_1\n",
    "        soft_update_params(\n",
    "            td3.critic_1, td3.target_critic_1, td3.cfg.algorithm.tau_target\n",
    "        )\n",
    "        # Application de la mise à jour douce pour critic_2 et target_critic_2\n",
    "        soft_update_params(\n",
    "            td3.critic_2, td3.target_critic_2, td3.cfg.algorithm.tau_target\n",
    "        )\n",
    "        \n",
    "        # --- Évaluation de l'acteur si nécessaire ---\n",
    "        if td3.evaluate():  # Si une évaluation est requise\n",
    "            if td3.cfg.plot_agents:  # Si la configuration requiert de générer des plots\n",
    "                # Génération des graphiques pour visualiser la politique de l'acteur\n",
    "                plot_policy(\n",
    "                    td3.actor,\n",
    "                    td3.eval_env,\n",
    "                    td3.best_reward,\n",
    "                    str(ddpg.base_dir / \"plots\"),\n",
    "                    td3.cfg.gym_env.env_name,\n",
    "                    stochastic=False,  # Pas de stochasticité lors de l'évaluation\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamètres principaux \n",
    "BATCH_SIZE = 64\n",
    "TAU_TARGET = 0.05\n",
    "DISCOUNT_FACTOR = 0.98\n",
    "LEARNING_RATE_ACTOR = 1e-3\n",
    "LEARNING_RATE_CRITIC = 1e-3\n",
    "ACTOR_HIDDEN_SIZE = [400, 300]  # Taille des couches cachées de l'acteur\n",
    "CRITIC_HIDDEN_SIZE = [400, 300]  # Taille des couches cachées des critiques\n",
    "ACTION_NOISE = 0.1\n",
    "MAX_GRAD_NORM = 0.5\n",
    "LEARNING_STARTS = 10000\n",
    "BUFFER_SIZE = int(2e5)  # Taille du replay buffer\n",
    "N_STEPS = 100  # Nombre de pas avant mise à jour\n",
    "NB_EVALS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des paramètres avec les hyperparamètres définis\n",
    "params = {\n",
    "    \"save_best\": False,\n",
    "    \"base_dir\": \"${gym_env.env_name}/td3-S${algorithm.seed}_${current_time:}\",\n",
    "    \"collect_stats\": True,\n",
    "    \"plot_agents\": True,\n",
    "    \"algorithm\": {\n",
    "        \"seed\": 1,\n",
    "        \"max_grad_norm\": MAX_GRAD_NORM,\n",
    "        \"epsilon\": 0.02,\n",
    "        \"n_envs\": 1,\n",
    "        \"n_steps\": N_STEPS,\n",
    "        \"nb_evals\": NB_EVALS,\n",
    "        \"discount_factor\": DISCOUNT_FACTOR,\n",
    "        \"buffer_size\": BUFFER_SIZE,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"tau_target\": TAU_TARGET,\n",
    "        \"eval_interval\": 2000,\n",
    "        \"max_epochs\": 11000,\n",
    "        \"learning_starts\": LEARNING_STARTS,\n",
    "        \"action_noise\": ACTION_NOISE,\n",
    "        \"architecture\": {\n",
    "            \"actor_hidden_size\": ACTOR_HIDDEN_SIZE,\n",
    "            \"critic_hidden_size\": CRITIC_HIDDEN_SIZE,\n",
    "        },\n",
    "    },\n",
    "    \"gym_env\": {\n",
    "        \"env_name\": \"Pendulum-v1\",\n",
    "    },\n",
    "    \"actor_optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": LEARNING_RATE_ACTOR,\n",
    "    },\n",
    "    \"critic_optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": LEARNING_RATE_CRITIC,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c71ae89f18ea43cdb1993e3b066802aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m td3 \u001b[38;5;241m=\u001b[39m TD3(OmegaConf\u001b[38;5;241m.\u001b[39mcreate(params))\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrun_td3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtd3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m td3\u001b[38;5;241m.\u001b[39mvisualize_best()\n",
      "Cell \u001b[0;32mIn[29], line 3\u001b[0m, in \u001b[0;36mrun_td3\u001b[0;34m(td3)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_td3\u001b[39m(td3: TD3):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Boucle principale d'apprentissage TD3, qui itère sur le replay buffer (mémoire d'expériences)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m rb \u001b[38;5;129;01min\u001b[39;00m td3\u001b[38;5;241m.\u001b[39miter_replay_buffers():\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;66;03m# Extraction d'un batch d'expériences aléatoires depuis le replay buffer\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         rb_workspace \u001b[38;5;241m=\u001b[39m rb\u001b[38;5;241m.\u001b[39mget_shuffled(td3\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39malgorithm\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;66;03m# Mise à jour des réseaux de critiques à t=0\u001b[39;00m\n",
      "File \u001b[0;32m~/ENTER/envs/deepdac/lib/python3.9/site-packages/bbrl_utils/algorithms.py:291\u001b[0m, in \u001b[0;36mEpochBasedAlgo.iter_replay_buffers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m     train_workspace\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    290\u001b[0m     train_workspace\u001b[38;5;241m.\u001b[39mcopy_n_last_steps(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_workspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstochastic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39malgorithm\u001b[38;5;241m.\u001b[39mn_steps \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39malgorithm\u001b[38;5;241m.\u001b[39mn_envs\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# Add transitions to buffer\u001b[39;00m\n",
      "File \u001b[0;32m~/ENTER/envs/deepdac/lib/python3.9/site-packages/bbrl/agents/utils.py:79\u001b[0m, in \u001b[0;36mTemporalAgent.__call__\u001b[0;34m(self, workspace, t, n_steps, stop_variable, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m _t \u001b[38;5;241m=\u001b[39m t\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_variable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m         s \u001b[38;5;241m=\u001b[39m workspace\u001b[38;5;241m.\u001b[39mget(stop_variable, _t)\n",
      "File \u001b[0;32m~/ENTER/envs/deepdac/lib/python3.9/site-packages/bbrl/agents/utils.py:31\u001b[0m, in \u001b[0;36mAgents.__call__\u001b[0;34m(self, workspace, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, workspace, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents:\n\u001b[0;32m---> 31\u001b[0m         \u001b[43ma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ENTER/envs/deepdac/lib/python3.9/site-packages/bbrl/agents/agent.py:84\u001b[0m, in \u001b[0;36mAgent.__call__\u001b[0;34m(self, workspace, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m workspace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Agent.__call__] workspace must not be None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkspace \u001b[38;5;241m=\u001b[39m workspace\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkspace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/ENTER/envs/deepdac/lib/python3.9/site-packages/bbrl/agents/gymnasium.py:480\u001b[0m, in \u001b[0;36mParallelGymAgent.forward\u001b[0;34m(self, t, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m         frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step(k, dict_slice(k, action))\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 480\u001b[0m         frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;66;03m# Use last frame\u001b[39;00m\n\u001b[1;32m    483\u001b[0m     frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_frame[k]\n",
      "File \u001b[0;32m~/ENTER/envs/deepdac/lib/python3.9/site-packages/bbrl/agents/gymnasium.py:433\u001b[0m, in \u001b[0;36mParallelGymAgent._step\u001b[0;34m(self, k, action)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timestep[k] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcumulated_reward[k] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m--> 433\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_obs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterminated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mterminated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreward\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ENTER/envs/deepdac/lib/python3.9/site-packages/bbrl/agents/gymnasium.py:392\u001b[0m, in \u001b[0;36mParallelGymAgent._format_obs\u001b[0;34m(self, k, obs, info, terminated, truncated, reward)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;66;03m# Use the final observation instead\u001b[39;00m\n\u001b[1;32m    383\u001b[0m     observation \u001b[38;5;241m=\u001b[39m ParallelGymAgent\u001b[38;5;241m.\u001b[39m_format_frame(info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_observation\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    385\u001b[0m ret: Dict[\u001b[38;5;28mstr\u001b[39m, Tensor] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mobservation,\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminated\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([terminated]),\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruncated\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([truncated]),\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([done]),\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([reward])\u001b[38;5;241m.\u001b[39mfloat(),\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcumulated_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcumulated_reward[k]]),\n\u001b[0;32m--> 392\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestep\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timestep\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    393\u001b[0m }\n\u001b[1;32m    395\u001b[0m \u001b[38;5;66;03m# Resets the cumulated reward and timestep\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_autoreset:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "td3 = TD3(OmegaConf.create(params))\n",
    "run_td3(td3)\n",
    "td3.visualize_best()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepdac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
